{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Importing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T11:17:17.442538Z",
     "start_time": "2018-10-15T11:17:11.760338Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "IS_OFFLine = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计缺失值函数\n",
    "\n",
    "def na_summary(dat):\n",
    "    result = pd.isnull(dat).sum().to_frame()\n",
    "    result.rename(columns={0:'count'}, inplace=True)\n",
    "    result['percent'] = (result['count'] / len(dat)).map(lambda x: format(x, '.1%'))\n",
    "    print(dat.shape, '\\n', '========================', '\\n', result)\n",
    "    \n",
    "def xgb_valid(train_set_x,train_set_y):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'silent':1,\n",
    "              'nthread':8\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(train_set_x, label=train_set_y)\n",
    "    model = xgb.cv(params, dtrain, num_boost_round=1000,nfold=5,metrics={'auc'},seed=10)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "def xgb_feature(train_set_x,train_set_y,test_set_x,test_set_y):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(train_set_x, label=train_set_y)\n",
    "    dvali = xgb.DMatrix(test_set_x)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=800)\n",
    "    predict = model.predict(dvali)\n",
    "    return predict, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-30T11:53:22.500099Z",
     "start_time": "2018-09-30T11:53:22.498094Z"
    }
   },
   "source": [
    "# Preprocessing Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_auth_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量构建\n",
    "\n",
    "# 统计“手机号”是否缺失（缺失为1，否则为0）      train_auth_phone_na   \n",
    "# 统计“身份证号”是否缺失（缺失为1，否则为0）    train_auth_id_card_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T11:17:38.678059Z",
     "start_time": "2018-10-15T11:17:38.279184Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_card</th>\n",
       "      <th>auth_time</th>\n",
       "      <th>phone</th>\n",
       "      <th>id</th>\n",
       "      <th>train_auth_phone_na</th>\n",
       "      <th>train_auth_id_card_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4****************0</td>\n",
       "      <td>2017-06-10</td>\n",
       "      <td>132****2008</td>\n",
       "      <td>501951980776722440</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>187*****500</td>\n",
       "      <td>525890212484616200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5****************9</td>\n",
       "      <td>2015-10-10</td>\n",
       "      <td>135****3522</td>\n",
       "      <td>599309364691472392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id_card  auth_time        phone                  id  \\\n",
       "0  4****************0 2017-06-10  132****2008  501951980776722440   \n",
       "1                 NaN        NaT  187*****500  525890212484616200   \n",
       "2  5****************9 2015-10-10  135****3522  599309364691472392   \n",
       "\n",
       "   train_auth_phone_na  train_auth_id_card_na  \n",
       "0                    1                      1  \n",
       "1                    1                      0  \n",
       "2                    1                      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_auth = pd.read_csv('AI_risk_train_V3.0/train_auth_info.csv', parse_dates = ['auth_time'])\n",
    "\n",
    "train_auth['train_auth_phone_na']   = train_auth['phone'].map(lambda x: 0 if str(x) == 'nan' else 1)\n",
    "train_auth['train_auth_id_card_na'] = train_auth['id_card'].map(lambda x: 0 if str(x) == 'nan' else 1)\n",
    "\n",
    "train_auth.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_bankcard_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量重塑\n",
    "\n",
    "# 统计每个id对应的“银行名称”的个数                train_bankcard_bankname_count\n",
    "# 统计每个id对应的“银行名称（去重）”的个数        train_bankcard_bankname_nunique\n",
    "# 统计每个id对应的“银行卡类型（去重）”的个数      train_bankcard_cardtype_nunique\n",
    "# 统计每个id对应的“手机号（去重）”的个数          train_bankcard_phone_nunique\n",
    "# 统计每个id对应的“银行卡号后四位（去重）”的个数  train_bankcard_tailnum_nunique    不明白这个变量的意义【删除】\n",
    "# 统计每个id对应的“银行卡绑定手机号”列表          train_bankcard_phone_list         目的是判断当前手机是否在手机号列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T11:18:32.970831Z",
     "start_time": "2018-10-15T11:18:32.318408Z"
    }
   },
   "outputs": [],
   "source": [
    "train_bankcard = pd.read_csv('AI_risk_train_V3.0/train_bankcard_info.csv')\n",
    "\n",
    "train_bankcard_bankname_count  =train_bankcard.groupby(by='id',as_index=False)['bank_name'].agg({'train_bankcard_bank_name_count': len})\n",
    "train_bankcard_bankname_nunique=train_bankcard.groupby(by='id',as_index=False)['bank_name'].agg({'train_bankcard_bank_name_nunique': 'nunique'})\n",
    "train_bankcard_cardtype_nunique=train_bankcard.groupby(by='id',as_index=False)['card_type'].agg({'train_bankcard_card_type_nunique': 'nunique'})\n",
    "train_bankcard_phone_nunique   =train_bankcard.groupby(by='id',as_index=False)['phone'].agg({'train_bankcard_phone_nunique': 'nunique'})\n",
    "train_bankcard_phone_list=train_bankcard.groupby(by='id',as_index=False)['phone'].agg({'train_bankcard_phone_list': lambda x: list(set(x.tolist()))})\n",
    "# train_bankcard_tailnum_nunique =train_bankcard.groupby(by='id',as_index=False)['tail_num'].agg({'train_bankcard_tailnum_nunique': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_creat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量构建\n",
    "\n",
    "# 最高分-当前分数          train_credit_score_inverse\n",
    "# 网购平台信用额度可用值   train_credit_can_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T11:21:43.221682Z",
     "start_time": "2018-10-15T11:21:42.872754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>overdraft</th>\n",
       "      <th>quota</th>\n",
       "      <th>id</th>\n",
       "      <th>train_credit_score_inverse</th>\n",
       "      <th>train_credit_can_use</th>\n",
       "      <th>train_credit_can_use_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>563888070781309192</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>591567740590887176</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>4955.9</td>\n",
       "      <td>5083.0</td>\n",
       "      <td>464888846169936136</td>\n",
       "      <td>115.0</td>\n",
       "      <td>127.1</td>\n",
       "      <td>0.974993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_score  overdraft   quota                  id  \\\n",
       "0         549.0        0.0     0.0  563888070781309192   \n",
       "1         400.0        0.0     0.0  591567740590887176   \n",
       "2         490.0     4955.9  5083.0  464888846169936136   \n",
       "\n",
       "   train_credit_score_inverse  train_credit_can_use  \\\n",
       "0                        56.0                   0.0   \n",
       "1                       205.0                   0.0   \n",
       "2                       115.0                 127.1   \n",
       "\n",
       "   train_credit_can_use_ratio  \n",
       "0                    0.000000  \n",
       "1                    0.000000  \n",
       "2                    0.974993  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_credit = pd.read_csv('AI_risk_train_V3.0/train_credit_info.csv')\n",
    "\n",
    "credit_score_max = max(train_credit.credit_score)\n",
    "train_credit['train_credit_score_inverse'] = train_credit['credit_score'].map(lambda x: credit_score_max-x)\n",
    "train_credit['train_credit_can_use'] = train_credit['quota'] - train_credit['overdraft']\n",
    "train_credit['train_credit_can_use_ratio'] = train_credit['overdraft'] / (train_credit['quota'] + 0.01)\n",
    "\n",
    "train_credit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_order_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量构建\n",
    "\n",
    "# 规范化“下单时间”\n",
    "# 统计每个id对应的最晚下单时间            train_order_time_max\n",
    "# 统计每个id对应的最早下单时间            train_order_time_min\n",
    "# 统计每个id对应“在线支付”的次数        train_order_type_pay_zaixian\n",
    "# 统计每个id对应“货到付款”的次数        train_order_type_pay_huodao\n",
    "# 统计每个id对应“购物”的次数            train_order_id_count\n",
    "# 统计每个id对应“订单金额”的均值        train_order_amt_order_mean\n",
    "# 统计每个id对应“商品单价”的均值        train_order_unit_price_mean\n",
    "# 统计每个id对应“下单”（去重）的次数    train_order_order_time_nunique   不明白其意义【删除】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T11:22:26.672004Z",
     "start_time": "2018-10-15T11:22:19.343390Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_order = pd.read_csv('AI_risk_train_V3.0/train_order_info.csv', parse_dates=['time_order'])\n",
    "\n",
    "train_order['time_order'] = train_order['time_order'] \\\n",
    "    .map(lambda x : pd.lib.NaT if (str(x) == '0' or x == 'NA' or x == 'nan')\n",
    "         else (datetime.datetime.strptime(str(x),'%Y-%m-%d %H:%M:%S') if ':' in str(x)\n",
    "               else (datetime.datetime.utcfromtimestamp(int(x[0:10])) + datetime.timedelta(hours = 8))))\n",
    "\n",
    "train_order_time_max = train_order.groupby(by='id', as_index=False)['time_order'].agg({'train_order_time_max': max})\n",
    "train_order_time_min = train_order.groupby(by='id', as_index=False)['time_order'].agg({'train_order_time_min': min})\n",
    "\n",
    "train_order_type_zaixian=train_order.groupby(by='id', as_index=False)['type_pay'].agg({'train_order_type_pay_zaixian': lambda x: sum(x == '在线支付')})\n",
    "train_order_type_huodao =train_order.groupby(by='id', as_index=False)['type_pay'].agg({'train_order_type_pay_huodao': lambda x: sum(x == '货到付款')})\n",
    "\n",
    "train_order_id_count = train_order.groupby(by = 'id', as_index=False)['id'].agg({'train_order_id_count': len})\n",
    "\n",
    "train_order_amt_order_mean  = train_order.groupby(by = 'id',as_index=False)['amt_order'].agg({'train_order_amt_order_mean': np.mean})\n",
    "train_order_unit_price_mean = train_order.groupby(by = 'id',as_index=False)['unit_price'].agg({'train_order_unit_price_mean': np.mean})\n",
    "# train_order_order_time_nunique = train_order.groupby(by = 'id',as_index=False)['time_order'].agg({'train_order_order_time_nunique': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_recieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量构建\n",
    "\n",
    "# 独热编码地区                                      train_recieve_new\n",
    "# 统计每个id对应的“固定收货手机号”的个数          train_recieve_phone_count   即此人固定收货手机号收货的次数\n",
    "# 统计每个id对应的“固定收货手机号”的个数（去重）  train_recieve_phone_nunique 即此人有多少固定收货手机号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T11:46:30.896414Z",
     "start_time": "2018-10-15T11:46:30.109038Z"
    }
   },
   "outputs": [],
   "source": [
    "train_recieve = pd.read_csv('AI_risk_train_V3.0/train_recieve_addr_info.csv')\n",
    "\n",
    "train_recieve['region'] = train_recieve['region'].map(lambda x: str(x)[:2])\n",
    "train_recieve_new = pd.crosstab(train_recieve.id, train_recieve.region).reset_index()\n",
    "\n",
    "train_recieve_phone_count   = train_recieve.groupby(by=['id'])['fix_phone'].agg({'train_recieve_phone_count': len})\n",
    "train_recieve_phone_nunique = train_recieve.groupby(by=['id'])['fix_phone'].agg({'train_recieve_phone_nunique': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('AI_risk_train_V3.0/train_target.csv', parse_dates = ['appl_sbm_tm'])\n",
    "\n",
    "train_target['hour']  = train_target['appl_sbm_tm'].map(lambda x: x.hour)\n",
    "train_target['month'] = train_target['appl_sbm_tm'].map(lambda x: x.month)\n",
    "train_target['year']  = train_target['appl_sbm_tm'].map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量构建\n",
    "\n",
    "# 统计兴趣变量是否缺失         is_hobby_na\n",
    "# 统计身份证号是否缺失         is_id_card_na\n",
    "# 规范化生日变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv('AI_risk_train_V3.0/train_user_info.csv')\n",
    "\n",
    "train_user['train_user_is_hobby_na'] = train_user['hobby'].map(lambda x: 0 if str(x) == 'nan' else 1)\n",
    "train_user['train_user_is_id_card_na'] = train_user['id_card'].map(lambda x:0 if str(x )== 'nan' else 1)\n",
    "\n",
    "\n",
    "tmp = train_user[['id','birthday']].set_index(['id'])\n",
    "\n",
    "train_user_is_double_ = tmp['birthday'].map(lambda x: (str(x) == '--')*1).reset_index(name='train_user_is_double_')\n",
    "train_user_is_0_0_0 = tmp['birthday'].map(lambda x:(str(x) == '0-0-0')*1).reset_index(name='train_user_is_0_0_0')\n",
    "train_user_is_1_1_1 = tmp['birthday'].map(lambda x:(str(x) == '1-1-1')*1).reset_index(name='train_user_is_1_1_1')\n",
    "train_user_is_0000_00_00 = tmp['birthday'].map(lambda x:(str(x) == '0000-00-00')*1).reset_index(name='train_user_is_0000_00_00')\n",
    "train_user_is_0001_1_1 = tmp['birthday'].map(lambda x:(str(x) == '0001-1-1')*1).reset_index(name='train_user_is_0001_1_1')\n",
    "train_user_is_hou_in = tmp['birthday'].map(lambda x:('后' in str(x))*1).reset_index(name='train_user_is_hou_in')\n",
    "\n",
    "train_user['birthday'] = train_user['birthday'] \\\n",
    "    .map(lambda x: datetime.datetime.strptime(str(x), '%Y-%m-%d') \n",
    "         if(re.match('19\\d{2}-\\d{1,2}-\\d{1,2}', str(x)) and '-0' not in str(x)) else pd.lib.NaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_target, train_auth, on=['id'], how='left')\n",
    "train_data = pd.merge(train_data, train_user, on=['id'], how='left')\n",
    "train_data = pd.merge(train_data, train_credit,on=['id'], how='left')\n",
    "train_data = pd.merge(train_data, train_bankcard_phone_list, on=['id'], how='left')\n",
    "\n",
    "train_data = pd.merge(train_data,train_bankcard_bankname_count,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_bankcard_bankname_nunique,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_bankcard_cardtype_nunique,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_bankcard_phone_nunique,on=['id'],how='left')\n",
    "\n",
    "train_data = pd.merge(train_data,train_order_time_max,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_order_time_min,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_order_type_zaixian,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_order_type_huodao,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_order_id_count,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_order_amt_order_mean,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_order_unit_price_mean,on=['id'],how='left')\n",
    "\n",
    "train_data = pd.merge(train_data,train_recieve_new,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_recieve_phone_count,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_recieve_phone_nunique,on=['id'],how='left')\n",
    "\n",
    "train_data = pd.merge(train_data,train_user_is_double_,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_user_is_0_0_0,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_user_is_1_1_1,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_user_is_0000_00_00,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_user_is_0001_1_1,on=['id'],how='left')\n",
    "train_data = pd.merge(train_data,train_user_is_hou_in,on=['id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 变量重塑\n",
    "\n",
    "# 统计每行缺失值的个数                            train_na_num\n",
    "# 统计auth_info和user_info里面的id_card是否一致   train_data_same_id_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['train_na_num'] = train_data.isnull().sum(axis=1)\n",
    "\n",
    "# create \"the_same_idcard\"  \n",
    "\n",
    "auth_idcard = list(train_data['id_card_x'])\n",
    "user_idcard = list(train_data['id_card_y'])\n",
    "\n",
    "idcard_result = []\n",
    "for indexx, uu in enumerate(auth_idcard):\n",
    "    \n",
    "    if (str(auth_idcard[indexx])=='nan') and (str(user_idcard[indexx])=='nan'):\n",
    "        idcard_result.append(0)\n",
    "        \n",
    "    elif (str(auth_idcard[indexx])!='nan') and (str(user_idcard[indexx])=='nan'):\n",
    "        idcard_result.append(1)\n",
    "        \n",
    "    elif (str(auth_idcard[indexx])=='nan') and (str(user_idcard[indexx])!='nan'):\n",
    "        idcard_result.append(2)\n",
    "        \n",
    "    else:\n",
    "        ttt1 = str(auth_idcard[indexx])[0] + str(auth_idcard[indexx])[-1]\n",
    "        ttt2 = str(user_idcard[indexx])[0] + str(user_idcard[indexx])[-1]\n",
    "        \n",
    "        if ttt1 == ttt2:\n",
    "            idcard_result.append(3)\n",
    "        if ttt1 != ttt2:\n",
    "            idcard_result.append(4)\n",
    "            \n",
    "train_data['train_data_same_id_card'] = idcard_result\n",
    "\n",
    "\n",
    "# 判断当前的手机号(train_auth)是否在手机号列表中，并剔除手机号列表\n",
    "train_data['train_data_exist_phone'] = train_data.apply(lambda x: x['phone'] in x['train_bankcard_phone_list'], axis=1)\n",
    "train_data['train_data_exist_phone'] = train_data['train_data_exist_phone']*1\n",
    "train_data = train_data.drop(['train_bankcard_phone_list'], axis=1)\n",
    "\n",
    "train_data['train_data_diff_day'] = train_data.apply(lambda x: (x['appl_sbm_tm'] - x['auth_time']).days, axis=1) # 贷款提交时间-认证时间\n",
    "train_data['train_data_age'] = train_data.apply(lambda x: (x['appl_sbm_tm'] - x['birthday']).days / 365, axis=1) # 用户年龄\n",
    "\n",
    "train_data['train_data_day_order_max'] = train_data.apply(lambda x: (x['appl_sbm_tm'] - x['train_order_time_max']).days, axis=1)\n",
    "train_data = train_data.drop(['train_order_time_max'], axis=1)\n",
    "\n",
    "train_data['train_data_day_order_min'] = train_data.apply(lambda row: (row['appl_sbm_tm'] - row['train_order_time_min']).days,axis=1)\n",
    "train_data = train_data.drop(['train_order_time_min'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_OFFLine == False:\n",
    "    train_data = train_data.drop(['appl_sbm_tm','id','id_card_x','auth_time','phone','birthday','hobby','id_card_y'],axis=1)\n",
    "\n",
    "# if IS_OFFLine == True:\n",
    "#     dummy_fea = ['sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry']\n",
    "#     dummy_df = pd.get_dummies(train_data.loc[:,dummy_fea])\n",
    "#     train_data_copy = pd.concat([train_data,dummy_df],axis=1)\n",
    "#     train_data_copy = train_data_copy.fillna(0)\n",
    "#     vaild_train_data = train_data_copy.drop(dummy_fea,axis=1)\n",
    "#     valid_train_train = vaild_train_data[vaild_train_data.appl_sbm_tm < datetime.datetime(2017,4,1)]\n",
    "#     valid_train_test = vaild_train_data[vaild_train_data.appl_sbm_tm >= datetime.datetime(2017,4,1)]\n",
    "#     valid_train_train = valid_train_train.drop(['appl_sbm_tm','id','id_card_x','auth_time','phone','birthday','hobby','id_card_y'],axis=1)\n",
    "#     valid_train_test = valid_train_test.drop(['appl_sbm_tm','id','id_card_x','auth_time','phone','birthday','hobby','id_card_y'],axis=1)\n",
    "#     vaild_train_x = valid_train_train.drop(['target'],axis=1)\n",
    "#     vaild_test_x = valid_train_test.drop(['target'],axis=1)\n",
    "#     redict_result, modelee = xgb_feature(vaild_train_x,valid_train_train['target'].values,vaild_test_x,None)\n",
    "#     print('valid auc',roc_auc_score(valid_train_test['target'].values,redict_result))\n",
    "#     sys.exit(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_all_1117.csv', encoding='gbk', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auth = pd.read_csv('AI_Risk_data_Btest_V2.0/test_auth_info.csv',parse_dates = ['auth_time'])\n",
    "\n",
    "test_auth['test_auth_id_card_na'] = test_auth['id_card'].map(lambda x: 0 if str(x) == 'nan' else 1)\n",
    "test_auth['test_auth_phone_na'] = test_auth['phone'].map(lambda x: 0 if str(x) == 'nan' else 1)\n",
    "\n",
    "test_auth['auth_time'].replace('0000-00-00', 'nan', inplace=True)\n",
    "test_auth['auth_time'] = pd.to_datetime(test_auth['auth_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_bankcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bankcard = pd.read_csv('AI_Risk_data_Btest_V2.0/test_bankcard_info.csv')\n",
    "\n",
    "test_bankcard_bankname_count  =test_bankcard.groupby(by='id',as_index=False)['bank_name'].agg({'test_bankcard_bank_name_count': len})\n",
    "test_bankcard_bankname_nunique=test_bankcard.groupby(by='id',as_index=False)['bank_name'].agg({'test_bankcard_bank_name_nunique': 'nunique'})\n",
    "test_bankcard_cardtype_nunique=test_bankcard.groupby(by='id',as_index=False)['card_type'].agg({'test_bankcard_card_type_nunique': 'nunique'})\n",
    "test_bankcard_phone_nunique   =test_bankcard.groupby(by='id',as_index=False)['phone'].agg({'test_bankcard_phone_nunique': 'nunique'})\n",
    "test_bankcard_phone_list=test_bankcard.groupby(by='id',as_index=False)['phone'].agg({'test_bankcard_phone_list': lambda x: list(set(x.tolist()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>overdraft</th>\n",
       "      <th>quota</th>\n",
       "      <th>id</th>\n",
       "      <th>test_credit_score_inverse</th>\n",
       "      <th>test_credit_can_use</th>\n",
       "      <th>test_credit_can_use_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>389.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>647995096096051464</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>647548977264201736</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>384.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>639787275877617928</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_score  overdraft  quota                  id  \\\n",
       "0         389.0        0.0    0.0  647995096096051464   \n",
       "1         400.0        0.0    0.0  647548977264201736   \n",
       "2         384.0        0.0    0.0  639787275877617928   \n",
       "\n",
       "   test_credit_score_inverse  test_credit_can_use  test_credit_can_use_ratio  \n",
       "0                      216.0                  0.0                        0.0  \n",
       "1                      205.0                  0.0                        0.0  \n",
       "2                      221.0                  0.0                        0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_credit = pd.read_csv('AI_Risk_data_Btest_V2.0/test_credit_info.csv')\n",
    "\n",
    "credit_score_max = max(test_credit.credit_score)\n",
    "test_credit['test_credit_score_inverse'] = test_credit['credit_score'].map(lambda x: credit_score_max-x)\n",
    "test_credit['test_credit_can_use'] = test_credit['quota'] - test_credit['overdraft']\n",
    "test_credit['test_credit_can_use_ratio'] = test_credit['overdraft'] / (test_credit['quota'] + 0.01)\n",
    "\n",
    "test_credit.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_order = pd.read_csv('AI_Risk_data_Btest_V2.0/test_order_info.csv', parse_dates=['time_order'])\n",
    "\n",
    "test_order['time_order'] = test_order['time_order'] \\\n",
    "    .map(lambda x : pd.lib.NaT if (str(x) == '0' or x == 'NA' or x == 'nan')\n",
    "         else (datetime.datetime.strptime(str(x),'%Y-%m-%d %H:%M:%S') if ':' in str(x)\n",
    "               else (datetime.datetime.utcfromtimestamp(int(x[0:10])) + datetime.timedelta(hours = 8))))\n",
    "\n",
    "test_order_time_max = test_order.groupby(by='id', as_index=False)['time_order'].agg({'test_order_time_max': max})\n",
    "test_order_time_min = test_order.groupby(by='id', as_index=False)['time_order'].agg({'test_order_time_min': min})\n",
    "\n",
    "test_order_type_zaixian=test_order.groupby(by='id', as_index=False)['type_pay'].agg({'test_order_type_pay_zaixian': lambda x: sum(x == '在线支付')})\n",
    "test_order_type_huodao =test_order.groupby(by='id', as_index=False)['type_pay'].agg({'test_order_type_pay_huodao': lambda x: sum(x == '货到付款')})\n",
    "\n",
    "test_order_id_count = test_order.groupby(by = 'id', as_index=False)['id'].agg({'test_order_id_count': len})\n",
    "\n",
    "test_order_amt_order_mean  = test_order.groupby(by = 'id',as_index=False)['amt_order'].agg({'test_order_amt_order_mean': np.mean})\n",
    "test_order_unit_price_mean = test_order.groupby(by = 'id',as_index=False)['unit_price'].agg({'test_order_unit_price_mean': np.mean})\n",
    "# test_order_order_time_nunique = test_order.groupby(by = 'id',as_index=False)['time_order'].agg({'test_order_order_time_nunique': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_recieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recieve = pd.read_csv('AI_Risk_data_Btest_V2.0/test_recieve_addr_info.csv')\n",
    "\n",
    "test_recieve['region'] = test_recieve['region'].map(lambda x: str(x)[:2])\n",
    "test_recieve_new = pd.crosstab(test_recieve.id, test_recieve.region).reset_index()\n",
    "\n",
    "test_recieve_phone_count   = test_recieve.groupby(by=['id'])['fix_phone'].agg({'test_recieve_phone_count': len})\n",
    "test_recieve_phone_nunique = test_recieve.groupby(by=['id'])['fix_phone'].agg({'test_recieve_phone_nunique': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_csv('AI_Risk_data_Btest_V2.0/test_list.csv',parse_dates = ['appl_sbm_tm'])\n",
    "\n",
    "test_target['hour']  = test_target['appl_sbm_tm'].map(lambda x: x.hour)\n",
    "test_target['month'] = test_target['appl_sbm_tm'].map(lambda x: x.month)\n",
    "test_target['year']  = test_target['appl_sbm_tm'].map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = pd.read_csv('AI_Risk_data_Btest_V2.0/test_user_info.csv',parse_dates = ['birthday'])\n",
    "\n",
    "test_user['test_user_is_hobby_na'] = test_user['hobby'].map(lambda x: 0 if str(x) == 'nan' else 1)\n",
    "test_user['test_user_is_id_card_na'] = test_user['id_card'].map(lambda x:0 if str(x )== 'nan' else 1)\n",
    "\n",
    "\n",
    "tmp = test_user[['id','birthday']].set_index(['id'])\n",
    "\n",
    "test_user_is_double_ = tmp['birthday'].map(lambda x: (str(x) == '--')*1).reset_index(name='test_user_is_double_')\n",
    "test_user_is_0_0_0 = tmp['birthday'].map(lambda x:(str(x) == '0-0-0')*1).reset_index(name='test_user_is_0_0_0')\n",
    "test_user_is_1_1_1 = tmp['birthday'].map(lambda x:(str(x) == '1-1-1')*1).reset_index(name='test_user_is_1_1_1')\n",
    "test_user_is_0000_00_00 = tmp['birthday'].map(lambda x:(str(x) == '0000-00-00')*1).reset_index(name='test_user_is_0000_00_00')\n",
    "test_user_is_0001_1_1 = tmp['birthday'].map(lambda x:(str(x) == '0001-1-1')*1).reset_index(name='test_user_is_0001_1_1')\n",
    "test_user_is_hou_in = tmp['birthday'].map(lambda x:('后' in str(x))*1).reset_index(name='test_user_is_hou_in')\n",
    "\n",
    "test_user['birthday'] = test_user['birthday'] \\\n",
    "    .map(lambda x: datetime.datetime.strptime(str(x), '%Y-%m-%d') \n",
    "         if(re.match('19\\d{2}-\\d{1,2}-\\d{1,2}', str(x)) and '-0' not in str(x)) else pd.lib.NaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.merge(test_target, test_auth, on=['id'], how='left')\n",
    "test_data = pd.merge(test_data, test_user, on=['id'], how='left')\n",
    "test_data = pd.merge(test_data, test_credit,on=['id'], how='left')\n",
    "test_data = pd.merge(test_data, test_bankcard_phone_list, on=['id'], how='left')\n",
    "\n",
    "test_data = pd.merge(test_data,test_bankcard_bankname_count,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_bankcard_bankname_nunique,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_bankcard_cardtype_nunique,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_bankcard_phone_nunique,on=['id'],how='left')\n",
    "\n",
    "test_data = pd.merge(test_data,test_order_time_max,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_order_time_min,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_order_type_zaixian,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_order_type_huodao,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_order_id_count,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_order_amt_order_mean,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_order_unit_price_mean,on=['id'],how='left')\n",
    "\n",
    "test_data = pd.merge(test_data,test_recieve_new,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_recieve_phone_count,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_recieve_phone_nunique,on=['id'],how='left')\n",
    "\n",
    "test_data = pd.merge(test_data,test_user_is_double_,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_user_is_0_0_0,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_user_is_1_1_1,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_user_is_0000_00_00,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_user_is_0001_1_1,on=['id'],how='left')\n",
    "test_data = pd.merge(test_data,test_user_is_hou_in,on=['id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['test_na_num'] = test_data.isnull().sum(axis=1)\n",
    "\n",
    "# create \"the_same_idcard\"  \n",
    "\n",
    "auth_idcard = list(test_data['id_card_x'])\n",
    "user_idcard = list(test_data['id_card_y'])\n",
    "\n",
    "idcard_result = []\n",
    "for indexx, uu in enumerate(auth_idcard):\n",
    "    \n",
    "    if (str(auth_idcard[indexx])=='nan') and (str(user_idcard[indexx])=='nan'):\n",
    "        idcard_result.append(0)\n",
    "        \n",
    "    elif (str(auth_idcard[indexx])!='nan') and (str(user_idcard[indexx])=='nan'):\n",
    "        idcard_result.append(1)\n",
    "        \n",
    "    elif (str(auth_idcard[indexx])=='nan') and (str(user_idcard[indexx])!='nan'):\n",
    "        idcard_result.append(2)\n",
    "        \n",
    "    else:\n",
    "        ttt1 = str(auth_idcard[indexx])[0] + str(auth_idcard[indexx])[-1]\n",
    "        ttt2 = str(user_idcard[indexx])[0] + str(user_idcard[indexx])[-1]\n",
    "        \n",
    "        if ttt1 == ttt2:\n",
    "            idcard_result.append(3)\n",
    "        if ttt1 != ttt2:\n",
    "            idcard_result.append(4)\n",
    "            \n",
    "test_data['test_data_same_id_card'] = idcard_result\n",
    "\n",
    "\n",
    "# 判断当前的手机号(test_auth)是否在手机号列表中，并剔除手机号列表\n",
    "test_data['test_data_exist_phone'] = test_data.apply(lambda x: x['phone'] in x['test_bankcard_phone_list'], axis=1)\n",
    "test_data['test_data_exist_phone'] = test_data['test_data_exist_phone']*1\n",
    "test_data = test_data.drop(['test_bankcard_phone_list'], axis=1)\n",
    "\n",
    "test_data['test_data_diff_day'] = test_data.apply(lambda x: (x['appl_sbm_tm'] - x['auth_time']).days, axis=1) # 贷款提交时间-认证时间\n",
    "test_data['test_data_age'] = test_data.apply(lambda x: (x['appl_sbm_tm'] - x['birthday']).days / 365, axis=1) # 用户年龄\n",
    "\n",
    "test_data['test_data_day_order_max'] = test_data.apply(lambda x: (x['appl_sbm_tm'] - x['test_order_time_max']).days, axis=1)\n",
    "test_data = test_data.drop(['test_order_time_max'], axis=1)\n",
    "\n",
    "test_data['test_data_day_order_min'] = test_data.apply(lambda row: (row['appl_sbm_tm'] - row['test_order_time_min']).days,axis=1)\n",
    "test_data = test_data.drop(['test_order_time_min'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['appl_sbm_tm','id','id_card_x','auth_time','phone','birthday','hobby','id_card_y'],axis=1)\n",
    "test_data['target'] = -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test_data_all_1117.csv', encoding='gbk', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = pd.concat([train_data, test_data], axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data.to_csv('train_test_data_all_1117.csv', encoding='gbk', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "account_grade                        object\n",
       "credit_score                        float64\n",
       "degree                               object\n",
       "hour                                  int64\n",
       "income                               object\n",
       "industry                             object\n",
       "merriage                             object\n",
       "month                                 int64\n",
       "na                                    int64\n",
       "overdraft                           float64\n",
       "qq_bound                             object\n",
       "quota                               float64\n",
       "sex                                  object\n",
       "target                                int64\n",
       "test_auth_id_card_na                float64\n",
       "test_auth_phone_na                  float64\n",
       "test_bankcard_bank_name_count       float64\n",
       "test_bankcard_bank_name_nunique     float64\n",
       "test_bankcard_card_type_nunique     float64\n",
       "test_bankcard_phone_nunique         float64\n",
       "test_credit_can_use                 float64\n",
       "test_credit_can_use_ratio           float64\n",
       "test_credit_score_inverse           float64\n",
       "test_data_age                       float64\n",
       "test_data_day_order_max             float64\n",
       "test_data_day_order_min             float64\n",
       "test_data_diff_day                  float64\n",
       "test_data_exist_phone               float64\n",
       "test_data_same_id_card              float64\n",
       "test_na_num                         float64\n",
       "test_order_amt_order_mean           float64\n",
       "test_order_id_count                 float64\n",
       "test_order_type_pay_huodao          float64\n",
       "test_order_type_pay_zaixian         float64\n",
       "test_order_unit_price_mean          float64\n",
       "test_recieve_phone_count            float64\n",
       "test_recieve_phone_nunique          float64\n",
       "test_user_is_0000_00_00             float64\n",
       "test_user_is_0001_1_1               float64\n",
       "test_user_is_0_0_0                  float64\n",
       "test_user_is_1_1_1                  float64\n",
       "test_user_is_double_                float64\n",
       "test_user_is_hobby_na               float64\n",
       "test_user_is_hou_in                 float64\n",
       "test_user_is_id_card_na             float64\n",
       "train_auth_id_card_na               float64\n",
       "train_auth_phone_na                 float64\n",
       "train_bankcard_bank_name_count      float64\n",
       "train_bankcard_bank_name_nunique    float64\n",
       "train_bankcard_card_type_nunique    float64\n",
       "train_bankcard_phone_nunique        float64\n",
       "train_credit_can_use                float64\n",
       "train_credit_can_use_ratio          float64\n",
       "train_credit_score_inverse          float64\n",
       "train_data_age                      float64\n",
       "train_data_day_order_max            float64\n",
       "train_data_day_order_min            float64\n",
       "train_data_diff_day                 float64\n",
       "train_data_exist_phone              float64\n",
       "train_data_same_id_card             float64\n",
       "train_na_num                        float64\n",
       "train_order_amt_order_mean          float64\n",
       "train_order_id_count                float64\n",
       "train_order_type_pay_huodao         float64\n",
       "train_order_type_pay_zaixian        float64\n",
       "train_order_unit_price_mean         float64\n",
       "train_recieve_phone_count           float64\n",
       "train_recieve_phone_nunique         float64\n",
       "train_user_is_0000_00_00            float64\n",
       "train_user_is_0001_1_1              float64\n",
       "train_user_is_0_0_0                 float64\n",
       "train_user_is_1_1_1                 float64\n",
       "train_user_is_double_               float64\n",
       "train_user_is_hobby_na              float64\n",
       "train_user_is_hou_in                float64\n",
       "train_user_is_id_card_na            float64\n",
       "wechat_bound                         object\n",
       "year                                  int64\n",
       "上海                                    int64\n",
       "云南                                    int64\n",
       "内蒙                                    int64\n",
       "北京                                    int64\n",
       "台湾                                    int64\n",
       "吉林                                    int64\n",
       "四川                                    int64\n",
       "天津                                    int64\n",
       "宁夏                                    int64\n",
       "安徽                                    int64\n",
       "山东                                    int64\n",
       "山西                                    int64\n",
       "广东                                    int64\n",
       "广西                                    int64\n",
       "新疆                                    int64\n",
       "江苏                                    int64\n",
       "江西                                    int64\n",
       "河北                                    int64\n",
       "河南                                    int64\n",
       "浙江                                    int64\n",
       "海南                                    int64\n",
       "港澳                                  float64\n",
       "湖北                                    int64\n",
       "湖南                                    int64\n",
       "甘肃                                    int64\n",
       "福建                                    int64\n",
       "西藏                                    int64\n",
       "贵州                                    int64\n",
       "辽宁                                    int64\n",
       "重庆                                    int64\n",
       "陕西                                    int64\n",
       "青海                                    int64\n",
       "香港                                    int64\n",
       "黑龙                                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data.fillna(0, inplace=True)\n",
    "\n",
    "## 对下面几个变量进行独热编码\n",
    "dummy_fea = ['sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound', 'account_grade', 'industry']\n",
    "dummy_df = pd.get_dummies(train_test_data.loc[:, dummy_fea])\n",
    "\n",
    "# 列合并独热编码的变量\n",
    "train_test_data = pd.concat([train_test_data, dummy_df], axis=1)\n",
    "train_test_data = train_test_data.drop(dummy_fea, axis=1)\n",
    "\n",
    "## 分开训练数据和测试数据\n",
    "train_data_new = train_test_data.iloc[:train_data.shape[0], :]\n",
    "test_data_new = train_test_data.iloc[train_data.shape[0]:, :]\n",
    "\n",
    "## 训练数据和测试数据都踢除target变量\n",
    "train_data_new_x = train_data_new.drop(['target'], axis=1)\n",
    "test_data_new_x = test_data_new.drop(['target'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "predict_result, modelee = xgb_feature(train_data_new_x, train_data_new['target'].values, test_data_new_x, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv('AI_Risk_data_Btest_V2.0/test_list.csv', parse_dates = ['appl_sbm_tm'])\n",
    "\n",
    "ans['PROB'] = predict_result\n",
    "ans = ans.drop(['appl_sbm_tm'], axis=1)\n",
    "minmin, maxmax = min(ans['PROB']), max(ans['PROB'])\n",
    "ans['PROB'] = ans['PROB'].map(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "ans['PROB'] = ans['PROB'].map(lambda x:'%.4f' % x)\n",
    "\n",
    "ans.to_csv('1117_result_test.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
